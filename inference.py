#!/usr/bin/env python3
"""
ç»Ÿä¸€æ¨ç†è„šæœ¬ - æ”¯æŒå•æ¬¡æµ‹è¯•å’Œæ‰¹é‡æµ‹è¯•
ä½¿ç”¨æ–¹æ³•: 
  python inference.py "ä½ çš„é—®é¢˜"
  python inference.py  # æ‰¹é‡æµ‹è¯•æ¨¡å¼
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import sys

SYSTEM_PROMPT = """æŒ‰ç…§å¦‚ä¸‹æ ¼å¼å›ç­”é—®é¢˜ï¼š
<think>
ä½ çš„æ€è€ƒè¿‡ç¨‹
</think>
<answer>
ä½ çš„å›ç­”
</answer>"""

def load_model(checkpoint_path="./output/checkpoint_20"):
    """åŠ è½½æ¨¡å‹"""
    print(f"ğŸ“¦ Loading model from {checkpoint_path}...", file=sys.stderr)
    
    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
    model = AutoModelForCausalLM.from_pretrained(
        checkpoint_path,
        torch_dtype=torch.float16
    )
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)
    model.eval()
    
    print(f"âœ… Model loaded on {device}\n", file=sys.stderr)
    return model, tokenizer, device

def generate(model, tokenizer, device, question):
    """ç”Ÿæˆå›å¤"""
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": question}
    ]
    
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer([text], return_tensors="pt").to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(
        outputs[0][inputs['input_ids'].shape[1]:], 
        skip_special_tokens=True
    )
    
    return response

def main():
    model, tokenizer, device = load_model()
    
    # å¦‚æœæä¾›äº†å‘½ä»¤è¡Œå‚æ•°ï¼Œåˆ™è¿›è¡Œå•æ¬¡æµ‹è¯•
    if len(sys.argv) > 1:
        question = " ".join(sys.argv[1:])
        print(f"â“ é—®é¢˜: {question}\n")
        response = generate(model, tokenizer, device, question)
        print(f"ğŸ’¡ å›ç­”:\n{response}")
    else:
        # æ‰¹é‡æµ‹è¯•æ¨¡å¼
        test_cases = [
            "å¤©ä¸Šäº”åªé¸Ÿï¼Œåœ°ä¸Šäº”åªé¸¡ï¼Œä¸€å…±å‡ åªé¸­ï¼Ÿ",
            "å°æ˜æœ‰10ä¸ªè‹¹æœï¼Œåƒäº†3ä¸ªï¼Œè¿˜å‰©å‡ ä¸ªï¼Ÿ",
            "1 + 1 = ?",
            "ä¸€ä¸ªæ•°åŠ ä¸Š5ç­‰äº12ï¼Œè¿™ä¸ªæ•°æ˜¯å¤šå°‘ï¼Ÿ",
        ]
        
        print("="*80)
        for i, question in enumerate(test_cases, 1):
            print(f"\n[æµ‹è¯• {i}/{len(test_cases)}]")
            print(f"â“ é—®é¢˜: {question}")
            print("-"*80)
            response = generate(model, tokenizer, device, question)
            print(f"ğŸ’¡ å›ç­”:\n{response}")
            print("="*80)

if __name__ == "__main__":
    main()

